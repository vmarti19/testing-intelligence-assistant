# üß† Visteon-Assistant-Intelligence (VAI)
Intelligent assistant designed to streamline access to relevant project information and extract specific parameters to optimize the implementation process in automated test cases for generic features.

## Problems

- Interpretation and analysis of feature implementation for automated test cases.
- Fast generation of generic parameter files (`.json`) for each warning or RTT.
- Optimize querying of relevant information in the database.
- Generation of test cases by interpreting pseudocode from JIRA-reported defects.

# Objective

Create an intelligent assistant to streamline access to relevant project information and extract specific parameters to optimize the implementation process in automated test cases for generic features such as warnings and indicators (TTs, RTTs).

# Methodology

## Flow Explanation

1. **Client ‚Üí Agent**: The user submits a request (e.g., a question or task).
2. **Agent ‚Üí MCP**: The agent decides how to handle the request and requests access to the model.
3. **MCP ‚Üí LLM**: The MCP routes the request to the language model (LLM).
4. **LLM ‚Üí MCP**: The model generates a response and returns it to the MCP.
5. **MCP ‚Üí Agent**: The MCP delivers the response to the agent.
6. **Agent ‚Üí Client**: The agent may process, enrich, or contextualize the response before returning it to the user.

---

## üìÖ Project Phases and Activities

---

### üß± **Phase I: Basic Architecture (LLM + Agent + MCP)**

**Description**: Development of the basic architecture of the intelligent assistant, integrating LLM, Agent, and MCP, with a local console-level client.

**Goal:** Build a functional console-level assistant that can receive instructions, reason, and perform basic actions.

<p align="center">
¬† <img src="URL_DE_LA_IMAGEN" alt="Client LLM Diagram" width="400"/>
</p>

<p align="center"><strong>Figure:</strong> Client (LLM): Using a model trained for tools, running on an API such as Ollama or OpenAI.</p>

#### Activities:
- Select the LLM model (OpenAI, Ollama, Mistral, etc.)
- Set up the local development environment (Python, Node.js, virtual environments, dependencies)
- Design the agent architecture (ReAct, AutoGPT, LangGraph, etc.)
- Integrate the LLM with the agent (via API or local runtime)
- Install and configure Playwright MCP
- Design the communication protocol between the agent and the MCP
- Test the basic flow: input ‚Üí reasoning ‚Üí action ‚Üí response

---

### üß∞ **Phase II: Tool Development for the Agent**

**Description**: Development of tools for the agent.

**Goal:** Extend the agent‚Äôs capabilities with specialized tools.

#### Activities:
- Design a tool interface (input/output schema, validation)
- Implement basic tools:
  - Web navigation
  - File reading
  - File writing
  - Shell command execution
- Integrate tools into the agent (Tool use + planning)
- Write unit tests for each tool
- Validate agent reasoning with tools (ReAct, Plan-and-Execute)

---

### üîå **Phase III: Integration with External Tools via MCP**

**Description**: Integrating functionalities to execute external tools via MCP.
**Goal:** Enable the agent to interact with external tools like databases and browsers.

#### Activities:
- Design MCP adapters for external tools
- Integrate PGAdmin/PostgreSQL using psycopg2 or SQLAlchemy
- Define access permissions and security policies
- Test SQL queries generated by the agent
- Validate full flow: question ‚Üí query ‚Üí response

<p align="center">
¬† <img src="URL_DE_LA_IMAGEN" alt="Client LLM Diagram" width="400"/>
</p>

<p align="center"><strong>Figure:</strong> Model Context Protocol (MCP): Connection to PGAdmin, e.g., using `libpsycopg2`, `SQLAlchemy`.</p>

---

### ‚òÅÔ∏è **Phase IV: Remote Server Deployment**

**Description**: Integration of the assistant on a remote server.

**Goal:** Make the assistant accessible from anywhere.

#### Activities:
- Choose a hosting provider (e.g., AWS, Azure, GCP)
- Configure the remote environment (e.g., Docker, NGINX)
- Deploy the assistant backend
- Set up secure access (HTTPS, authentication)
- Test remote access from a local client

---

### üìö **Phase V: RAG Integration (Retrieval-Augmented Generation)**

**Description**: Integration of RAG*, connection to the remote server where RAG will be hosted. 

**Goal:** Enable the assistant to answer questions using external documents.

#### Activities:
- Select a RAG framework (LangChain, LlamaIndex, etc.)
- Connect to data sources (e.g., PDFs, knowledge bases)
- Index documents and build a vector store
- Integrate RAG as a tool in the agent
- Test questions with and without context retrieval

<p align="center">
¬† <img src="URL_DE_LA_IMAGEN" alt="Client LLM Diagram" width="400"/>
</p>

<p align="center"><strong>Figure:</strong> Retrieval-Augmented Generation (RAG): Implementation using frameworks like `langchain` or `llama-index`.</p>

(*) Reuse of RAG implementation by the DiApps team.

---

### üåê **Phase VI: Web Client Interface on Testrack**

**Description**: Development of a client interface integrated into the Testrack webpage for server access. Integrate the chatbot interface on the server at `www.testrack.visteon.com`.

**Goal:** Develop a user-friendly web interface for interacting with the assistant.

#### Activities:
- Design the chatbot UI (HTML/CSS/JS or React/Vue)
- Connect frontend to backend (REST API or WebSocket)
- Implement user authentication
- Deploy on [www.testrack.visteon.com](http://www.testrack.visteon.com)
- Conduct usability and performance testing

---

# Documentation

Code will be locally documented via docstrings and type hints; the linter will enforce this.
Implementation details and design information will be tracked via this table of contents.

|     | Section                               | Description                                                    |
| --- | ---------------------------------     | -------------------------------------------------------------- |
|     | __Backend__                           |                                                                |
| 1.1 | [Requirements](docs/requirements.md)  | Project dependencies                                           |
| 1.2 | [Source control](docs/git.md)         | Git commands reference                                         |

